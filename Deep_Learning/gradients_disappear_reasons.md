# 深度学习训练中梯度消失的原因有哪些？有哪些解决方法？

## 梯度消失产生的主要原因有：一是使用了深层网络，二是采用了不合适的损失函数。

(1) 目前优化神经网络的方法都是基于BP(Back Propogation, 反向传播)，
即根据损失函数计算的误差，通过梯度反向传播的方式，指导深度神经网络权重的更新优化。
将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助。

链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题
一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式
对深层网络权值进行更新时，得到的梯度接近于0，也就是梯度消失。

(2) 计算权重更新信息时，需要计算前层的偏导信息，因此如果激活函数选择不合适，比如使用了
sigmoid，梯度消失就会很明显。因为 sigmoid 的梯度是不会超过0.25，这样经过链式求导后，
很容易发生梯度消失。

## 解决办法

(1) pre-training + fine-tuning

Hinton 在2006年发表了一篇文章，为了解决梯度消失问题，提出采取无监督逐层训练方法，
其基本思想是每次训练一层隐藏节点，训练时将上一层隐藏节点的输出作为输入，
而本层隐藏节点的输出作为下一层隐藏节点的输入，此过程就是逐层预训练(pre-training)。
在训练完成功后，再对整个网络进行微调(fine-tuning)。此思想相当于是先寻找局部最优，
再整合起来寻找全局最优，此方法有一定的好处，但目前应用不多。

(2) 选择 relu 等梯度大部分落在常数上的激活函数

relu 函数的导数在正数部分是恒等于1的，因此在深层网络中，使用relu激活函数可以有效避免梯度消失的问题。

(3) batch normalization

BN 就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，
进而解决了梯度消失的问题，可以理解为 BN 将输出从饱和区拉到了非饱和区。

(4) 残差网络的捷径 (shortcut)

相比于之前的网络结构，残差网络中有很多跨层连接结构(shortcut)，
这样的结构在反向传播时多了反向传播的路径，
可以在一定程度上解决梯度消失的问题。

(5) LSTM 的门结构

LSTM 全称是长短时记忆网络(long short term memory networks)，LSTM 的结构设计可以改善 RNN 中的梯度消失的问题。
主要原因在于 LSTM 内部复杂的门(gates)，LSTM 通过它内部的门，可以在更新的时候记住前几次训练的残留记忆。
